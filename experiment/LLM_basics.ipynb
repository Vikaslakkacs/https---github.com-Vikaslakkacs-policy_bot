{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream go to therapy? Because it had too many sprinkles of self-doubt!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': None,\n",
       " 'first': {'name': None,\n",
       "  'input_variables': ['topic'],\n",
       "  'input_types': {},\n",
       "  'output_parser': None,\n",
       "  'partial_variables': {},\n",
       "  'metadata': None,\n",
       "  'tags': None,\n",
       "  'messages': [{'prompt': {'name': None,\n",
       "     'input_variables': ['topic'],\n",
       "     'input_types': {},\n",
       "     'output_parser': None,\n",
       "     'partial_variables': {},\n",
       "     'metadata': None,\n",
       "     'tags': None,\n",
       "     'template': 'tell me a short joke about {topic}',\n",
       "     'template_format': 'f-string',\n",
       "     'validate_template': False,\n",
       "     '_type': 'prompt'},\n",
       "    'additional_kwargs': {}}],\n",
       "  'validate_template': False,\n",
       "  '_type': 'chat'},\n",
       " 'middle': [{'model_name': 'gpt-3.5-turbo',\n",
       "   'model': 'gpt-3.5-turbo',\n",
       "   'stream': False,\n",
       "   'n': 1,\n",
       "   'temperature': 0.7,\n",
       "   '_type': 'openai-chat'}],\n",
       " 'last': {'name': None, '_type': 'default'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value= prompt.invoke({'topic':\"ice cream\"})\n",
    "prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='tell me a short joke about ice cream')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: tell me a short joke about ice cream'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let dig into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why did the ice cream break up with the spoon? Because it couldn't gelato-ver their differences!\")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message= model.invoke(prompt_value)\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm= ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "क्वांटम मैकेनिक्स एक भौतिकीकी क्षेत्र है जो छोटे अदम्य रसायनिक कणों के व्यवहार का अध्ययन करता है।\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    AIMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "messages=[\n",
    "    SystemMessage(content=\"You are a Physicist and respond only in hindi\"),\n",
    "    HumanMessage(content= \"Explain quantum mechanics in one sentence.\")\n",
    "]\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching LLM responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In Memory  cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm= ChatOpenAI(model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.1 ms, sys: 2.21 ms, total: 13.3 ms\n",
      "Wall time: 1.13 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two-tired!\")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.cache import InMemoryCache\n",
    "set_llm_cache(InMemoryCache())\n",
    "prompt= \"Tell me a joke that a toddler can understand\"\n",
    "llm.invoke(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets make same prompt and ask LLm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 422 µs, sys: 11 µs, total: 433 µs\n",
      "Wall time: 433 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two-tired!\")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "prompt= \"Tell me a joke that a toddler can understand\"\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Streaming\n",
    "**Instead of providing answer all at once,  LLm Streaming provides graphical interface with typing feature just as we see in chatgpt answers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm= ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Verse 1:\\nIn the still of the night, under the moon's soft glow\\nA raven takes flight, its wings black as coal\\nSilent and mysterious, it calls out to the sky\\nA creature of the night, soaring so high\\n\\nChorus:\\nMoon and raven, dancing in the night\\nA haunting melody, a beautiful sight\\nTheir spirits intertwined, in the darkness they roam\\nMoon and raven, forever alone\\n\\nVerse 2:\\nThe moon watches over, as the raven flies\\nA guardian above, with endless skies\\nThe raven's caw echoes, through the silent air\\nA song of sorrow, a melody rare\\n\\nChorus:\\nMoon and raven, dancing in the night\\nA haunting melody, a beautiful sight\\nTheir spirits intertwined, in the darkness they roam\\nMoon and raven, forever alone\\n\\nBridge:\\nTogether they roam, in the dead of night\\nTwo souls intertwined, in a ghostly flight\\nThe moon shines bright, the raven's wings unfurl\\nA bond unbreakable, in this darkened world\\n\\nChorus:\\nMoon and raven, dancing in the night\\nA haunting melody, a beautiful sight\\nTheir spirits intertwined, in the darkness they roam\\nMoon and raven, forever alone\\n\\nOutro:\\nMoon and raven, a tale as old as time\\nA bond unbreakable, in the night they shine\\nForever dancing, in the moon's soft glow\\nMoon and raven, in the darkness they know.\")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt= \"Write a song about Moon and a Raven\"\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets enable Streaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verse 1:\n",
      "Underneath the silver moon\n",
      "A raven flies, singing a haunting tune\n",
      "Its wings black as night, its eyes a piercing blue\n",
      "Guided by the moonlight, it knows just what to do\n",
      "\n",
      "Chorus:\n",
      "Moon and raven, dancing in the night\n",
      "A mystical bond, a beautiful sight\n",
      "Together they soar, through the darkened sky\n",
      "A timeless connection, that will never die\n",
      "\n",
      "Verse 2:\n",
      "The moon whispers secrets to the raven's ear\n",
      "Tales of love, loss, and fear\n",
      "The raven caws in response, a language of its own\n",
      "A symphony of darkness, a melody unknown\n",
      "\n",
      "Chorus:\n",
      "Moon and raven, dancing in the night\n",
      "A mystical bond, a beautiful sight\n",
      "Together they soar, through the darkened sky\n",
      "A timeless connection, that will never die\n",
      "\n",
      "Bridge:\n",
      "In the stillness of the night, they find solace and peace\n",
      "A bond that transcends time, a love that will never cease\n",
      "Moon and raven, forever intertwined\n",
      "In the vast expanse of the night sky, their spirits aligned\n",
      "\n",
      "Chorus:\n",
      "Moon and raven, dancing in the night\n",
      "A mystical bond, a beautiful sight\n",
      "Together they soar, through the darkened sky\n",
      "A timeless connection, that will never die\n",
      "\n",
      "Outro:\n",
      "Moon and raven, a celestial pair\n",
      "A symbol of mystery, of magic in the air\n",
      "In the darkness of the night, they will always be\n",
      "A reminder of the beauty in the unseen."
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "template= '''You are an expert virologist.\n",
    "Write a few sentences about following virus \"{virus}\" in {language}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template= PromptTemplate.from_template(template= template)\n",
    "\n",
    "prompt= prompt_template.format(virus='hiv', language='english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an expert virologist.\\nWrite a few sentences about following virus \"hiv\" in english\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm= ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "output= llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIV, or Human Immunodeficiency Virus, is a retrovirus that attacks the immune system, specifically targeting CD4 cells. This virus weakens the immune system over time, making individuals more susceptible to infections and diseases. If left untreated, HIV can progress to AIDS (Acquired Immunodeficiency Syndrome), which is a more advanced stage of the disease. It is important for individuals to get tested regularly and seek treatment if diagnosed with HIV to manage the virus and prevent further complications.\n"
     ]
    }
   ],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You respond only in JSON format.'), HumanMessage(content='Top 10 countries in Asia by population.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "chat_template= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content= 'You respond only in JSON format.'),\n",
    "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population.')\n",
    "    ]\n",
    ")\n",
    "messages= chat_template.format_messages(n='10', area='Asia')\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"1\": \"China\",\n",
      "    \"2\": \"India\",\n",
      "    \"3\": \"Indonesia\",\n",
      "    \"4\": \"Pakistan\",\n",
      "    \"5\": \"Bangladesh\",\n",
      "    \"6\": \"Japan\",\n",
      "    \"7\": \"Philippines\",\n",
      "    \"8\": \"Vietnam\",\n",
      "    \"9\": \"Iran\",\n",
      "    \"10\": \"Turkey\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm= ChatOpenAI()\n",
    "output= llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'virus': 'HIV', 'language': 'english', 'text': '- HIV (Human Immunodeficiency Virus) attacks the immune system, specifically targeting CD4 cells which are crucial for fighting off infections.\\n- It is transmitted through contact with infected bodily fluids such as blood, semen, vaginal fluids, and breast milk.\\n- HIV can lead to AIDS (Acquired Immunodeficiency Syndrome) if left untreated, which is a severe stage of the infection where the immune system is severely compromised.\\n- There is currently no cure for HIV, but antiretroviral therapy (ART) can help manage the virus and prevent the progression to AIDS.\\n- Prevention methods include practicing safe sex, using clean needles, and getting tested regularly.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "llm= ChatOpenAI()\n",
    "template=''' you are an experienced Virologist.\n",
    "Write few points about the following virus \"{virus}\" in \"{language}\". Try to summarize in bullet points.\n",
    "\n",
    "'''\n",
    "\n",
    "prompt_template= PromptTemplate.from_template(template= template)\n",
    "\n",
    "chain= LLMChain(\n",
    "    llm= llm,\n",
    "    prompt= prompt_template\n",
    ")\n",
    "output= chain.invoke({'virus': 'HIV', 'language':'english'})\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- HIV (Human Immunodeficiency Virus) attacks the immune system, specifically targeting CD4 cells which are crucial for fighting off infections.\n",
      "- It is transmitted through contact with infected bodily fluids such as blood, semen, vaginal fluids, and breast milk.\n",
      "- HIV can lead to AIDS (Acquired Immunodeficiency Syndrome) if left untreated, which is a severe stage of the infection where the immune system is severely compromised.\n",
      "- There is currently no cure for HIV, but antiretroviral therapy (ART) can help manage the virus and prevent the progression to AIDS.\n",
      "- Prevention methods include practicing safe sex, using clean needles, and getting tested regularly.\n"
     ]
    }
   ],
   "source": [
    "print(output['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- HIV వైరస్ మానవ ప్రజాలో స్థిరమైన ఆరోగ్య సమస్యలను కలిగిస్తుంది.\n",
      "- హివ్ వైరస్ ప్రజల రక్తం ద్వారా వ్యాప్తి అవుతుంది.\n",
      "- హివ్ సంక్రమణం రక్తం, అన్ని లోహాలలో కంటే హెచ్చరికగా జరుగుతుంది.\n",
      "- హివ్ వైరస్ కరిగిపోవడంలో సంక్రమణం పాత్రమైన వ్యక్తులు ఆరోగ్యకర నిరీక్షణలు చేయాలి.\n",
      "- హివ్ వైరస్ నివారణకు ప్రతిసారించడం, నిరోధక తాగుతుంది.\n"
     ]
    }
   ],
   "source": [
    "output= chain.invoke({'virus': 'HIV', 'language':'telugu'})\n",
    "\n",
    "print(output['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_How ever simple chains are useful to pass very less data. If you want to do more complex tasks, \n",
    "it is necessary to have **Sequentialchains**_\n",
    "\n",
    "### Sequential chains\n",
    "\n",
    "> We can use Sequential chains to pass output of one chain to input of another chain.\n",
    "\n",
    "2 types of Sequential chains:\n",
    "- Simple Sequential chains\n",
    "- General form of Sequential chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Sequential chains**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm1= ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.8)\n",
    "template1= '''  You are an experienced scientist and python programmer.\n",
    "Write a function that implements the concept of \"{concept}\".\n",
    "'''\n",
    "prompt_template1= PromptTemplate.from_template(template= template1)\n",
    "chain1= LLMChain(llm=llm1, prompt= prompt_template1)\n",
    "\n",
    "\n",
    "llm2= ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1.3)\n",
    "template2= ''' \n",
    "Given the python function \"{function}\", describe it as detailed as possible.\n",
    "'''\n",
    "prompt_template2= PromptTemplate.from_template(template=template2)\n",
    "chain2= LLMChain(llm=llm2, prompt= prompt_template2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mSure! Here is a Python function that implements linear regression:\n",
      "\n",
      "```python\n",
      "def linear_regression(x, y):\n",
      "    n = len(x)\n",
      "    sum_x = sum(x)\n",
      "    sum_y = sum(y)\n",
      "    sum_x_squared = sum([i**2 for i in x])\n",
      "    sum_xy = sum([x[i] * y[i] for i in range(n)])\n",
      "\n",
      "    m = (n * sum_xy - sum_x * sum_y) / (n * sum_x_squared - sum_x**2)\n",
      "    b = (sum_y - m * sum_x) / n\n",
      "\n",
      "    return m, b\n",
      "```\n",
      "\n",
      "You can use this function by passing in two lists `x` and `y` containing the input data points. The function will return the slope `m` and y-intercept `b` of the best-fit line for the data.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThis Python function `linear_regression` implements simple linear regression, a method used to find the best fitting straight line through a set of data points. \n",
      "\n",
      "Here's a breakdown of what the function does:\n",
      "- It takes two input parameters `x` and `y`, which should be lists of data points where `x` represents the independent variable and `y` represents the dependent variable.\n",
      "- It calculates the length of the input data points list `x` and stores it in the variable `n`.\n",
      "- It calculates the sum of all elements in the `x` list and stores it in the variable `sum_x`.\n",
      "- It calculates the sum of all elements in the `y` list and stores it in the variable `sum_y`.\n",
      "- It calculates the sum of squared elements in the `x` list and stores it in the variable `sum_x_squared`.\n",
      "- It calculates the sum of the product of corresponding elements in the `x` and `y` lists and stores it in the variable `sum_xy`.\n",
      "- It then uses the formula for calculating the slope `m` and y-intercept `b` of the best-fit line by:\n",
      "  - Calculating the slope `m` using the formula = (n * sum_xy - sum_x * sum_y) / (n * sum_x_squared - sum_x^2)\n",
      "  - Calculating the y-intercept `b` using the formula = (sum_y - m * sum_x) / n\n",
      "- It returns the calculated slope `m` and y-intercept `b`.\n",
      "\n",
      "You can use this function by providing two lists of data points representing the independent variable `x` and dependent variable `y`, and the function will output the slope and y-intercept of the best-fit line through the data.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "### We will define sequential template\n",
    "overall_chain= SimpleSequentialChain(\n",
    "    chains=[chain1, chain2],\n",
    "    verbose=True\n",
    ")\n",
    "output= overall_chain.invoke('linear regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'linear regression', 'output': \"This Python function `linear_regression` implements simple linear regression, a method used to find the best fitting straight line through a set of data points. \\n\\nHere's a breakdown of what the function does:\\n- It takes two input parameters `x` and `y`, which should be lists of data points where `x` represents the independent variable and `y` represents the dependent variable.\\n- It calculates the length of the input data points list `x` and stores it in the variable `n`.\\n- It calculates the sum of all elements in the `x` list and stores it in the variable `sum_x`.\\n- It calculates the sum of all elements in the `y` list and stores it in the variable `sum_y`.\\n- It calculates the sum of squared elements in the `x` list and stores it in the variable `sum_x_squared`.\\n- It calculates the sum of the product of corresponding elements in the `x` and `y` lists and stores it in the variable `sum_xy`.\\n- It then uses the formula for calculating the slope `m` and y-intercept `b` of the best-fit line by:\\n  - Calculating the slope `m` using the formula = (n * sum_xy - sum_x * sum_y) / (n * sum_x_squared - sum_x^2)\\n  - Calculating the y-intercept `b` using the formula = (sum_y - m * sum_x) / n\\n- It returns the calculated slope `m` and y-intercept `b`.\\n\\nYou can use this function by providing two lists of data points representing the independent variable `x` and dependent variable `y`, and the function will output the slope and y-intercept of the best-fit line through the data.\"}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Python function `linear_regression` implements simple linear regression, a method used to find the best fitting straight line through a set of data points. \n",
      "\n",
      "Here's a breakdown of what the function does:\n",
      "- It takes two input parameters `x` and `y`, which should be lists of data points where `x` represents the independent variable and `y` represents the dependent variable.\n",
      "- It calculates the length of the input data points list `x` and stores it in the variable `n`.\n",
      "- It calculates the sum of all elements in the `x` list and stores it in the variable `sum_x`.\n",
      "- It calculates the sum of all elements in the `y` list and stores it in the variable `sum_y`.\n",
      "- It calculates the sum of squared elements in the `x` list and stores it in the variable `sum_x_squared`.\n",
      "- It calculates the sum of the product of corresponding elements in the `x` and `y` lists and stores it in the variable `sum_xy`.\n",
      "- It then uses the formula for calculating the slope `m` and y-intercept `b` of the best-fit line by:\n",
      "  - Calculating the slope `m` using the formula = (n * sum_xy - sum_x * sum_y) / (n * sum_x_squared - sum_x^2)\n",
      "  - Calculating the y-intercept `b` using the formula = (sum_y - m * sum_x) / n\n",
      "- It returns the calculated slope `m` and y-intercept `b`.\n",
      "\n",
      "You can use this function by providing two lists of data points representing the independent variable `x` and dependent variable `y`, and the function will output the slope and y-intercept of the best-fit line through the data.\n"
     ]
    }
   ],
   "source": [
    "print(output['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain agents\n",
    "_Langchains are powerfull in generating text, translating languages and ansewring questions._\n",
    "\n",
    "\n",
    "**But they cannot have logical sense and calulative sense**\n",
    "\n",
    "#### This is when Langchain agents comes into place where it will use thir party tools to achieve the tasks that are asigned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "policy_bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
